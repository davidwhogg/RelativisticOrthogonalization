% Copyright 2021 the authors. All rights reserved.

% To-do
% -----
% - Search for all HOGG, SOLE, \cite{} and fix them.
% - Stronger literature searches.
% - Do we say v_1,v_2,\ldots,v_n or do we say v_j, 1\leq j\leq n? Decide and flow it through.

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsfonts}
\usepackage{biblatex}
\addbibresource{sr.bib}

% math definitions
\newcommand{\inner}[2]{\langle{#1},{#2}\rangle}
\newcommand{\bra}[1]{\langle\,{#1}\,|}
\newcommand{\ket}[1]{|\,{#1}\,\rangle}
\newcommand{\braket}[2]{\langle\,{#1}\,|\,{#2}\,\rangle}
\newcommand{\ketbra}[2]{|\,{#1}\,\rangle\,\langle\,{#2}\,|}

% fixing latex page layout and typography
\setlength{\textwidth}{5.50in}
\setlength{\textheight}{9.40in}
\setlength{\oddsidemargin}{3.25in}
\addtolength{\oddsidemargin}{-0.5\textwidth}
\setlength{\topmargin}{-0.40in}
\renewcommand{\small}{\normalsize} % pure evil
\linespread{1.08}
\frenchspacing\raggedbottom\sloppy\sloppypar
\pagestyle{myheadings}
\markboth{}{\textsf{Hogg \& Villar / Orthogonalization in relativity}}
\newcommand{\documentname}{\textsl{Note}}
\newcommand{\secref}[1]{Section~\ref{#1}}

\title{\bfseries Orthogonalization and vector operations in special and general relativity\footnote{%
It is a pleasure to thank Ben Blum-Smith (NYU) for very valuable discussions. This research was supported in part by XXX YYY.}}
\author{\textbf{David W. Hogg}\\
        \textsl{Flatiron Institute}\\
        \and
        \textbf{Soledad Villar}\\
        \textsl{Johns Hopkins University}}
\date{incomplete draft as of 2021 June 05}

\begin{document}\thispagestyle{plain}
\maketitle

\begin{abstract}\noindent
    Special and general relativity in $d+1$ dimensions (our macroscopic Universe is $3+1$) can be thought of as metric theories with a metric that isn't positive definite, such that timelike, spacelike, and lightlike vectors have positive, negative, and zero magnitudes.
    These spaces violate of a lot of the intuitions we have about subspaces, inner products, and orthogonality.
    The concept of orthogonalization (Gram--Schmidt, for example) carries over to Minkowski/Lorentz space, but there are pathologies in which it is possible for the procedure to produce a lightlike vector and thus bork.
    We describe how to avoid this and successfully orthogonalize any $d+1$ linearly independent input vectors; every successful such orthogonalization produces one time-like and $d$ space-like unit vectors, all orthogonal (in the Minkowski sense).
    We use these orthogonalizations to construct coordinate-free representations of Lorentz transformations that fix or align particular vectors or subspaces.
    That is, we effectively present \emph{a new parameterization of the Lorentz transformations} for all $d+1$.
    As in many areas of physics, important aspects of relativity get simpler when we think of the theory in terms of vectors and coordinate-free forms built therefrom.
\end{abstract}

\section{Introduction}\label{sec:intro}

Special relativity~\cite{sr} is a purely kinematic theory, which can be formulated in terms of vector displacements (4-vectors in $(3+1)$-dimensional spacetime) between \emph{events}.
The coordinate system can be varied not just by rotations and translations, but also by \emph{boosts}, in which the stationary observer (the zero-point of 3-velocity) is redefined.
Most of the remarkable and counter-intuitive aspects of special relativity---time dilation, length contraction, twin paradox, and so on---flow from the fact that boost transformations preserve not the usual vector magnitudes and inner products, but instead magnitudes and inner products made with a \emph{non-positive-definite metric tensor}\footnote{For some, you can't call something a ``metric tensor'' if it isn't positive definite: How can you use a metric to define displacements if those displacements won't satisfy the triangle inequality? But that's where we are.}.
General relativity~\cite{gr} is a dynamical theory in which the non-positive-definite spacetime metric becomes (in general) a function of space and time, and the resulting curvature of spacetime is related to the energy densities and stresses in the matter and fields.
This \documentname{} is motivated partly by attempts to understand the geometric properties of spacetime with this strange metric, and confront a few of the pathologies that arise.

For group theorists, $(3+1)$-dimensional special relativity is equivariant with respect to the group called the Lorentz group and denoted O(1,3).
In $(d+1)$-dimensional spacetime the group would be O(1,$d$).
The operators in the group O(1,$d$) include rotations in the $d$-dimensional spatial subspace of spacetime, reflections in space and time, and boosts.
These groups are generalizations of the orthogonal group O($d$), which is the group of rotations and reflections in $d$-dimensional space.
In the context of group theory, the word ``equivariant'' means the following:
If you rotate, reflect, or boost the space, all of the predictions of the theory rotate, reflect, and boost appropriately to match.
What group theorists call ``equivariance'', physicists call ``symmetry''.

HOGG...Why are we working on all this? Machine-learning angle.

HOGG...Why are we working on all this? Coordinate-free angle.

\paragraph{Our contribution:}
We do XXX and we do YYY. Make references to particular equations in particular sections.

\paragraph{Prior work:}
Some of the ideas discussed here have been looked at previously.
In particular, orthogonalization in special relativity is addressed in~\cite{joot}.
However, this previous work does not deal with the pathologies that can arise when orthogonalization is performed on arbitrary collections of vectors.
A kind of coordinate-free Lorentz transformations are developed in~\cite{wagner}, but with a very different notation (in terms of 3-vectors, not 4-vectors) and a very different set of goals (HOGG CHECK THIS).

\section{Orthogonalization, projection, and rotation in O($d$)}\label{sec:od}

Before we consider special and general relativity, it is worth reviewing how orthogonalizations are performed and projection and rotation operators are constructed in ordinary space with an ordinary metric.
In standard $d$-dimensional space, with the standard Euclidean metric (the identity), containing vectors governed by the standard orthogonal group O($d$), inner products (scalar products) of vectors are defined as follows:
Given two column vectors $u,v\in\mathbb{R}^d$ (or, more specifically, $\mathbb{R}^{d\times1}$), the inner product $\inner{u}{v}\in\mathbb{R}$ is defined as
\begin{align}
    \inner{u}{v} &= u^\top v = v^\top u ~.
\end{align}
Two vectors $u,v$ are considered orthogonal if their inner product vanishes, or $\inner{u}{v}=0$.

Imagine that we are given a collection of $n\leq d$ linearly independent vectors $v_1,v_2,\ldots,v_n$,
and we want to construct orthonormal basis vectors $\hat{u}_1,\hat{u}_2,\ldots,\hat{u}_n$ that span the linear subspace spanned by the vectors $v_j$.
We can perform this orthonormalization by the Gram--Schmidt process (SOLE: WHAT TO CITE HERE?):
We sequentially construct orthogonal vectors $u_1,u_2,\ldots,u_n$ and normalize them into orthogonal unit vectors $\hat{u}_1,\hat{u}_2,\ldots,\hat{u}_n$ by the following algorithm:
\begin{align}
    u_1 &\leftarrow v_1 \label{eq:ogs1}
    \\
    \mbox{then for each $j$ ($2\leq j\leq n$) in order:} ~~ u_j &\leftarrow v_j - \sum_{k=1}^{j-1} \frac{\inner{v_j}{u_k}}{\inner{u_k}{u_k}}\,u_k \label{eq:ogs2}
    \\
    \mbox{then for each $j$ ($1\leq j\leq n$):} ~~ \hat{u}_j &\leftarrow \frac{1}{\sqrt{\inner{u_j}{u_j}}}\,u_j ~. \label{eq:ogs3}
\end{align}
Note that the procedure in \eqref{eq:ogs2} is order-dependent: If you put the vectors $v_1,v_2,\ldots,v_n$ into a different order, the orthogonalization will return different specific basis unit vectors $\hat{u}_1,\hat{u}_2,\ldots,\hat{u}_n$.
However, all the different possible returned bases (under permutations of the $v_j$) will span the same $n$-dimentional linear subspace of the $d$-dimentional space.

If you have $n\leq d$ linearly independent vectors $v_j\in\mathbb{R}^d$ that span an $n$-dimensional subspace of the $d$-dimensional space, you can use this orthonormalization procedure to build a subspace projection operator $\Pi\in\mathbb{R}^{d\times d}$
\begin{align}
    \Pi &= \sum_{j=1}^n \hat{u}_j\,\hat{u}_j^\top ~,
\end{align}
where the $\hat{u}_j$ are the orthogonal unit vectors\footnote{Note that because---for us---all vectors $u,v$ are column vectors, $u\,v^\top\in\mathbb{R}^{d\times d}$ while $u^\top v\in\mathbb{R}$.} delivered by the orthonormalization procedure applied to the $v_j$.
This projection operator has the property that, for any vector $w\in\mathbb{R}^d$, the vector $\Pi\,w$ will lie in the subspace spanned by the $v_j$.
One consequence of this line of reasoning is that if you have $n=d$ linearly independent vectors $v_j$, they will span the whole space, and the projection operator $\Pi$ will become the identity matrix.
More properly for what follows, the projection operator will become \emph{the metric of the space}.
That is, if you have an orthonormal basis of $d$ unit vectors $\hat{u}_j$, the $d\times d$ Euclidean metric of the space can be written as
\begin{align}
    I &= \sum_{j=1}^d \hat{u}_j\,\hat{u}_j^\top = \sum_{j=1}^d \frac{u_j\,u_j^\top}{\inner{u_j}{u_j}} ~,
\end{align}
where we have written it in terms of both the normalized and un-normalized vectors, anticipating results to come.
Physicists might call these forms for $\Pi$ and $I$ \emph{coordinate-free}:
They are coordinate-free in the sense that they are stated just in terms of the input vectors, not in terms of matrix components.
Or, equivalently, we didn't have to specify my coordinate system when we constructed them; we only had to specify the vectors that span the subspace.

These aren't the only kinds of operators that can be written in this coordinate-free form.
It is also possible to write any rotation-reflection operator $Q$ in a coordinate-free form.
First a definition: A matrix $Q\in\mathbb{R}^{d\times d}$ is a rotation-reflection operator in the orthogonal group O($d$) if the operator preserves all inner products (scalar products).
That is,
\begin{equation}
    Q \in \mbox{O($d$)} ~ \mbox{if and only if} ~ \inner{u}{v}=\inner{Q\,u}{Q\,v} ~ \mbox{for all $u,v$ in $\mathbb{R}^d$} ~ .
\end{equation}
This, in turn, will be true if and only if $Q$ is a square root of the metric---the identity:
\begin{equation}
    Q \in \mbox{O($d$)} ~ \mbox{if and only if} ~ Q^\top Q=I ~ .
\end{equation}
We call $Q$ a rotation-reflection operator because the group encompasses all rotations, all reflections, and all combinations of rotations and reflections.

For example, the simplest (interesting) orthogonal space is $O(2)$. In this case the operators form a one-dimensional family; they are all of the form
\begin{align}
    Q &= \begin{bmatrix}\cos{\theta} & -\sin{\theta} \\ \sin{\theta} & \cos{\theta}\end{bmatrix} ~\mbox{or}~
    \begin{bmatrix}-\cos{\theta} & \sin{\theta} \\ \sin{\theta} & \cos{\theta}\end{bmatrix} \label{eq:o2}
    \\
    -\pi &< \theta < \pi ~,
\end{align}
where $\theta$ is a rotation angle.
The two choices in \eqref{eq:o2} correspond to, in one case, pure rotation and, in the other, rotation plus reflection.
When $d>2$ the coordinate representations of the $Q$ become large matrices full of trig functions.
They get very complicated, while the coordinate-free representations we are about to deliver stay simple.

Now how do we construct general coordinate-free forms for the operators $Q$?
Usually they are given in terms of sines and cosines of rotation angles, plus sign flips.
However, there are coordinate-free forms.
For example: Imagine that we have two complete orthonormal bases in $\mathbb{R}^d$, $\hat{u}_1,\hat{u}_2,\ldots,\hat{u}_d$ and $\hat{u}'_1,\hat{u}'_2,\ldots,\hat{u}'_d$.
We can now imagine a rotation-reflection operator $Q$ that rotates and reflects any vector $w$ in the same way that these two bases are rotated and reflected with respect to one another.
This particular operator $Q$ can be constructed by the following coordinate-free outer-product construction:
\begin{align}
    Q &= \sum_{j=1}^d \hat{u}'_j\,\hat{u}_j^\top ~.
\end{align}
That is, rotations and reflections can be specified directly using orthonormal bases, with no explicit reference to any angles.

Here's an irrelevant aside:
If your background is quantum mechanics, then you probably use bra--ket notation.
For you an inner product is written as a braket $\braket{u}{v}$, and an outer product is written as a ketbra $\ketbra{u}{v}$.
In this notation, the rotation operator $Q$ can be written 
\begin{align}
    Q &= \sum_{j=1}^d \ketbra{\hat{u}'_j}{\hat{u}_j} ~,
\end{align}
which looks just like a change-of-basis operator in quantum mechanics (see, for example,~\cite{}).

HOGG: For example: SHOW A FIGURE AND GIVE AN EXAMPLE IN O(2).
It should give an example in coordinate-free form, and then compare
to the standard matrix formulation.

\section{Relativistic notation}\label{sec:notation}

There is a history of notation in special and general relativity.
Here we deliver a translation from traditional Einstein summation notation, and traditional language about boost, to a more linear-algebra-oriented notation.
After this Section, we will be using exclusively the linear-algebra notation, which is simpler (for us).

In Lorentz or Minkowski space, we think of there being a metric $\Lambda$ (often called $g_{\mu\nu}$), which is a $(d+1)\times(d+1)$ matrix that is \emph{not} positive definite.
The metric $\Lambda$ is diagonal with $+1$ in the first position and $-1$ repeated on all the remaining $d$ diagonal elements.
In $3+1$ this is
\begin{align}\label{eq:sig}
    \Lambda &= \begin{bmatrix}1 & 0 & 0 & 0\\
                              0 & -1 & 0 & 0\\
                              0 & 0 & -1 & 0\\
                              0 & 0 & 0 & -1\end{bmatrix} ~.
\end{align}
There is another possible signature---with $-1$ in the first position and $+1$ thereafter.
In this \documentname{} we choose the signature illustrated in \eqref{eq:sig}; nothing significant in our discussion changes if you choose the opposite signature.
Indeed, this is one of the main reasons to pursue the coordinate-free representations pursued here:
The coordinate-free representations don't care about the signature of your metric, or any other aspects of your coordinate system.

Given two vectors $u$, $v$ in $d+1$, old-school relativists tend to write the relativistic inner product $\inner{u}{v}\in\mathbb{R}$ (the scalar product or Minkowski inner product) as
\begin{align}
    \inner{u}{v} &= u^\mu\,v_\mu = v^\mu\,u_\mu ~,
\end{align}
where $\mu$ is a component index\footnote{In this \documentname{}, greek indexes like $\mu$, $\nu$ will be indexes over vector components (going from 1 to 4 in $3+1$, for example), and roman indexes like $i$, $j$ will be indexes over vectors or other things.} going from 1 to $d+1$, and the repeated index is (implicitly) summed.
The $u^\mu$ is a contravariant vector component and the $v_\mu$ is a covariant vector component.
Contravariant and covariant components are related as follows:
\begin{align}
    u^\mu &= \Lambda^{\mu\nu}\,u_\nu \equiv \sum_{\nu=1}^{d+1} \Lambda^{\mu\nu}\,u_\nu
    \\
    \inner{u}{v} &= u^\mu\,v_\mu = \Lambda^{\mu\nu}\,u_\mu\,v_\nu \equiv \sum_{\mu=1}^{d+1}\sum_{\nu=1}^{d+1} \Lambda^{\mu\nu}\,u_\mu\,v_\nu
\end{align}
where the $\Lambda^{\mu\nu}$ are the components of the metric $\Lambda$.
The implicit summations on the left of the ``$\equiv$'' signs are guided by the rules of what's called Einstein summation notation (\cite{summation}; it is a subset of the Ricci calculus~\cite{ricci}): Indexes can appear exactly once or exactly twice (and no more) and when they appear twice, one must be up and one must be down, and they are summed from 1 to $d+1$.
In quantum-mechanics (bra--ket) notation, a covariant vector is like a ket $\ket{v}$, a contravariant vector is like a bra $\bra{v}$,
an inner product is a braket $\braket{u}{v}$, and an outer product (to appear below) is a ketbra $\ketbra{u}{v}$.

In linear algebra notation, if we think of $u$ and $v$ as being column vectors in $\mathbb{R}^{d+1}$ (or, to be extremely specific, $\mathbb{R}^{(d+1)\times 1}$), then we can write this same inner product as
\begin{align}\label{eq:inner}
    \inner{u}{v} &= u^\top\Lambda\,v ~.
\end{align}
We are going to use this notation going forward, not the Einstein summation notation.

In standard special-relativity lore, Lorentz transformations are taught as \emph{boost transformations} in which the assignment of the stationary observer is changed and the time and space axes change accordingly.
For our purposes, the Lorentz transformations also include spatial rotations, spatial reflections, and even time reflections (gasp!):
That is, for our purposes, an operator $Q$ is a valid Lorentz transformation if it is a member of the Lorentz group O(1,$d$).
To be more specific, an operator $Q$ (which can be thought of as a $(d+1)\times(d+1)$ matrix) is a Lorentz transformation---is in O(1,$d$)---if it preserves all inner products (scalar products). 
That is,
\begin{equation}
    Q \in \mbox{O(1,$d$)} ~ \mbox{if and only if} ~ \inner{u}{v}=\inner{Q\,u}{Q\,v} ~ \mbox{for all $u,v$ in $\mathbb{R}^{d+1}$} ~ .
\end{equation}
This, in turn, will be true if and only if $Q$ leaves the metric unchanged:
\begin{equation}
    Q \in \mbox{O(1,$d$)} ~ \mbox{if and only if} ~ Q^\top\Lambda\,Q=\Lambda ~ .
\end{equation}
This is our (surprising, perhaps) definition of the Lorentz transformation $Q$.

For example, the simplest Lorentz space is O(1,1) or $1+1$ ($d=1$).
In this case, the Lorentz transformations form a one-dimensional family; they are all of the form
\begin{align}
    Q &= \begin{bmatrix}\gamma & \beta\,\gamma \\ \beta\,\gamma & \gamma\end{bmatrix} ~\mbox{or}~
    \begin{bmatrix}-\gamma & -\beta\,\gamma \\ \beta\,\gamma & \gamma\end{bmatrix} ~\mbox{or}~
    \begin{bmatrix}\gamma & \beta\,\gamma \\ -\beta\,\gamma & -\gamma\end{bmatrix}  ~\mbox{or}~
    \begin{bmatrix}-\gamma & -\beta\,\gamma \\ -\beta\,\gamma & -\gamma\end{bmatrix} \label{eq:o11}
    \\
    \gamma &\equiv \frac{1}{\sqrt{1 - \beta^2}}
    \\
    -1 &< \beta < 1
\end{align}
In this simple case, $\beta$ is the dimensionless velocity of the boost, and $\gamma$ is the Lorentz factor.
The four choices in \eqref{eq:o11} correspond to even parity, reversing time (after the boost), reversing space (after the boost), and reversing both.
But in what follows we are going to develop \emph{coordinate-free forms} for the Lorentz transformations, in analogy to the coordinate-free forms for the rotation-reflection operators given in \secref{sec:od}.

\section{Relativistic orthogonalization}\label{sec:orth}

Before we orthogonalize, we should make some comments about vectors in O(1,$d$).
The first is that a vector $v$ can come in three forms now, \emph{timelike}, \emph{spacelike}, or \emph{lightlike} (sometimes \emph{null}):
\begin{align}
    v ~ \mbox{is timelike} &~ \mbox{if} ~ \inner{v}{v} > 0 \\
    v ~ \mbox{is lightlike} &~ \mbox{if} ~ \inner{v}{v} = 0 \\
    v ~ \mbox{is spacelike} &~ \mbox{if} ~ \inner{v}{v} < 0 ~.
\end{align}
(Technically, these definitions depend on the signature you chose for your metric; you would use the opposite language if you chose the opposite signature; see \secref{sec:notation}.)
Because Lorentz transformations $Q\in$~O(1,$d$) preserve inner products, you can't Lorentz transform a vector from any one of these three categories to any other category.

The second comment is that we will still say that two vectors are ``orthogonal'' if their inner product is zero.
\begin{align}
    u,v ~\mbox{are orthogonal} ~ &\mbox{if and only if} ~ \inner{u}{v}=0 ~,
\end{align}
where the inner product is defined as in \eqref{eq:inner}.
This is great, but it leads to a strange consequence:
Any lightlike vector $v$ is \emph{orthogonal to itself}.

The third comment is that linear dependence and independence carries over exactly from O($d$):
A set of $n$ vectors $v_j\in\mathbb{R}^{d+1}$ are linearly independent if there is no non-trivial set of amplitudes $a_j\in\mathbb{R}$ such that $\sum_{j=1}^n a_n\,v_n = 0$

Now: How does orthogonalization and orthonormalization work in relativity?
Once again you start with $n\leq d+1$ linearly independent vectors $v_1,v_2,\ldots,v_n$ in $\mathbb{R}^{d+1}$.
If these vectors are randomly generated, then with probability unity the straightforward generalization of Gram--Schmidt will work:
\begin{align}
    u_1 &\leftarrow v_1 \label{eq:rgs1}
    \\
    \mbox{then for each $j$ ($2\leq j\leq n$) in order:} ~~ u_j &\leftarrow v_j - \sum_{k=1}^{j-1} \frac{\inner{v_j}{u_k}}{\inner{u_k}{u_k}}\,u_k \label{eq:rgs2}
    \\
    \mbox{then for each $j$ ($1\leq j\leq n$):} ~~ \hat{u}_j &\leftarrow \frac{1}{\sqrt{|\inner{u_j}{u_j}|}}\,u_j ~, \label{eq:rgs3}
\end{align}
where we had to insert an absolute value inside the square-root operation because negative inner products will appear.
We're done. \emph{Ta-da!}
But we aren't really done, because the iteration step \eqref{eq:rgs2} involves dividing by an inner product, and inner products can vanish if the procedure accidentally produces a lightlike vector $u_j$ for some $j$.

In principle (and in practice if your context is adversarial), the orthogonalization procedure in \eqref{eq:rgs1} and \eqref{eq:rgs2} can produce a lightlike vector.
If you start with $n$ linearly independent vectors $v_j$, ($1\leq j\leq n$), begin the orthogonalization procedure, and then at some $j$ produce a lightlike $u_j$, here are the strategies:
\begin{enumerate}
    \item If all your input vectors $v_j$ are lightlike, then by construction, $u_1$ will be lightlike.
    Replace the first two input vectors $v_1, v_2$ with two (possibly randomly constructed) independent linear combinations of the $v_1$ and $v_2$.
    Re-start the orthogonalization.
    With high probability the orthogonalization will succeed.
    \item If not every one of your input vectors is lightlike, but the first one is, then by construction $u_1$ will be lightlike.
    Reorder the vectors such that the first vector is not lightlike and re-start the orthogonalization.
    With high probability the orthogonalization will succeed.
    \item If none of your input vectors is lightlike, but a lightlike $u_j$ is appearing, reorder the input vectors and re-start the orthogonalization.
    \item If all of the above fails, replace the $n$ input vectors $v_j$ with a randomly generated set of $n$ indepdent linear combinations of the original $n$ input vectors.
    With probability one, orthogonalization will succeed on this new set of inputs (and, by construction, it will span the same subspace as the original input vectors).
\end{enumerate}
These rules aren't extremely satisfying, because (unless you are extremely careful) they involve changing things like the orientation of the coordinate system.
We conjecture that there are better possible rules.

One comment to make is that orthogonalization is not necessarily numerically stable.
That is, a set of vectors can be formally linearly independent, but only barely (or not at all) at finite precision.
It is out of scope for us here, but there are methods to improve the stability of orthogonalizations (SOLE WHAT TO CITE?); these will be generalizable to the Lorentz space.

Another comment to make is that if you start with $n=d+1$ linearly independent vectors in $\mathbb{R}^{d+1}$, the outcome of the orthogonalization will be exactly one timelike vector and exactly $d$ spacelike vectors.
Importantly for what follows, \emph{the algorithm-generated set of normalized unit vectors $\hat{u}_1,\hat{u}_2,\ldots,\hat{u}_{d+1}$ will form a coordinate basis in spacetime}, with one time axis vector and three orthogonal spatial axis vectors.

HOGG...Construct projection operators.

HOGG...Construct the metric. Maybe comment one more time on the signature here?

HOGG...Remind the reader of the Einstein summation version, the bra--ket version, but use the linear-algebra version.

\section{Vector-guided Lorentz transformations}\label{sec:lt}

HOGG...Consider two observers moving through spacetime with 4-momenta $p_1,p_2$.
What do their coordinate systems look like?
What are their time axes?
What are their spatial axes?

HOGG...Construct the LT from $p_1, p_2$.

HOGG...Show what you would have written in Einstein summation notation and in bra-ket notation.

HOGG: FOR EXAMPLE: SHOW A FIGURE AND GIVE THE FORM FOR AN EXAMPLE IN O(1,1).

\section{Discussion}\label{sec:discussion}

HOGG: Much of this is very mathematical and theoretical.
However, the new parameterization of the Lorentz transformation in \secref{sec:lt} is useful and conceptually valuable.

HOGG: The hacks to fix the orthogonalization in \secref{sec:orth} are unsatisfying. Are there better solutions?

SOLE: What can we say about spaces like O(2,9), which might appear in string-like theories?

\raggedright
\printbibliography
\end{document}
